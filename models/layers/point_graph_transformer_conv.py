import math
from typing import Dict, List, Optional, Union

import torch
import torch.nn.functional as F
from torch import Tensor
from torch.nn import Parameter
from torch_sparse import SparseTensor
from torch import nn

from torch_geometric.nn.conv import MessagePassing
from torch_geometric.nn.dense import Linear
from torch_geometric.nn.inits import glorot, ones, reset
from torch_geometric.nn.module_dict import ModuleDict
from torch_geometric.nn.parameter_dict import ParameterDict
from torch_geometric.typing import EdgeType, Metadata, NodeType
from torch_geometric.utils import softmax


def group(xs: List[Tensor], aggr: Optional[str]) -> Optional[Tensor]:
    if len(xs) == 0:
        return None
    elif aggr is None:
        return torch.stack(xs, dim=1)
    elif len(xs) == 1:
        return xs[0]
    else:
        out = torch.stack(xs, dim=0)
        out = getattr(torch, aggr)(out, dim=0)
        out = out[0] if isinstance(out, tuple) else out
        return out


class PGHTConv(MessagePassing):
    r"""The Heterogeneous Graph Transformer (HGT) operator from the
    `"Heterogeneous Graph Transformer" <https://arxiv.org/abs/2003.01332>`_
    paper.

    .. note::

        For an example of using HGT, see `examples/hetero/hgt_dblp.py
        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/
        hetero/hgt_dblp.py>`_.

    Args:
        in_channels (int or Dict[str, int]): Size of each input sample of every
            node type, or :obj:`-1` to derive the size from the first input(s)
            to the forward method.
        out_channels (int): Size of each output sample.
        metadata (Tuple[List[str], List[Tuple[str, str, str]]]): The metadata
            of the heterogeneous graph, *i.e.* its node and edge types given
            by a list of strings and a list of string triplets, respectively.
            See :meth:`torch_geometric.data.HeteroData.metadata` for more
            information.
        group (string, optional): The aggregation scheme to use for grouping
            node embeddings generated by different relations.
            (:obj:`"sum"`, :obj:`"mean"`, :obj:`"min"`, :obj:`"max"`).
            (default: :obj:`"sum"`)
        **kwargs (optional): Additional arguments of
            :class:`torch_geometric.nn.conv.MessagePassing`.
    """
    def __init__(
        self,
        in_channels: Union[int, Dict[str, int]],
        edge_in_channels: Union[int, Dict[str, int]],
        out_channels: int,
        metadata: Metadata = None,
        group: str = "sum",
        num_attn_groups: int = 2,
        dropout = 0.1,
        simplify: bool = True,
        **kwargs,
    ):
        super().__init__(aggr='add', node_dim=0, **kwargs)

        assert out_channels % num_attn_groups == 0 , 'out_channels must be divisible by num_attn_groups'
        self._is_simplify = simplify
        metadata = [('node',),(('node', 'edge', 'node'),)] 
        if not isinstance(in_channels, dict):
            in_channels = {node_type: in_channels for node_type in metadata[0]}

        if not isinstance(edge_in_channels, dict):
            edge_in_channels = {edge_type: edge_in_channels for edge_type in metadata[1]}

        self.in_channels = in_channels
        self.edge_in_channels = edge_in_channels
        self.out_channels = out_channels
        self.group = group
        self.num_attn_groups = num_attn_groups
        self.k_lin = ModuleDict()
        self.q_lin = ModuleDict()
        self.v_lin = ModuleDict()
        self.out_lin = ModuleDict()
        self.identity_lin = ModuleDict()
        self.skip = ParameterDict()
        self.norm_input = ModuleDict()
        self.norm_output = ModuleDict()
        for node_type, in_channels in self.in_channels.items():
            self.norm_input[node_type] = nn.BatchNorm1d(in_channels)
            self.norm_output[node_type] = nn.BatchNorm1d(out_channels)
            self.k_lin[node_type] = nn.Sequential(nn.Linear(in_channels, out_channels),nn.BatchNorm1d(out_channels,affine=False), nn.ReLU())
            self.q_lin[node_type] = nn.Sequential(nn.Linear(in_channels, out_channels),nn.BatchNorm1d(out_channels,affine=False), nn.ReLU())
            self.v_lin[node_type] = Linear(in_channels, out_channels)
            self.out_lin[node_type] = Linear(out_channels, out_channels)
            self.skip[node_type] = Parameter(torch.Tensor(1))
            if in_channels != out_channels:
                self.identity_lin[node_type] = Linear(in_channels, out_channels)

        self.edge_lin = ModuleDict()
        for edge_type, in_channels in self.edge_in_channels.items():
            edge_type = '__'.join(edge_type)
            self.edge_lin[edge_type] = nn.Sequential(nn.Linear(in_channels, out_channels),nn.BatchNorm1d(out_channels), nn.ReLU(), nn.Linear(out_channels, out_channels))

        self.coords_bias_nn = nn.Sequential(nn.Linear(3, out_channels),nn.BatchNorm1d(out_channels), nn.ReLU(), nn.Linear(out_channels, out_channels))
        self.coords_mul_nn = nn.Sequential(nn.Linear(3, out_channels),nn.BatchNorm1d(out_channels), nn.ReLU(),nn.Linear(out_channels, out_channels))

        self.attn_nn = ModuleDict()
        self.norm_messages = ModuleDict()
        for edge_type, in_channels in self.edge_in_channels.items():
            edge_type = '__'.join(edge_type)
            self.norm_messages[edge_type]= nn.BatchNorm1d(out_channels)
            self.attn_nn[edge_type] = nn.Sequential(nn.Linear(out_channels, num_attn_groups),nn.BatchNorm1d(num_attn_groups), nn.ReLU(),nn.Linear(num_attn_groups, num_attn_groups))
        self.reset_parameters()
        self.attn_drop = nn.Dropout(dropout)
        if self._is_simplify:
            params_before = len(list(self.parameters()))
            self._simplify()
            params_after = len(list(self.parameters()))
            print(f'Simplifying: {params_before} -> {params_after}')

    def _simplify(self):
        params_dicts = [
            self.k_lin,
            self.q_lin,
            self.v_lin,
            self.out_lin,
            self.skip,
            self.norm_input, 
            self.norm_output,
            self.edge_lin,
            self.attn_nn,
            self.norm_messages
        ]
        if len(self.identity_lin) > 0:
            params_dicts.append(self.identity_lin)
        for param_dict in params_dicts:
            first_param = None
            for k, param in param_dict.items():
                if first_param is None:
                    first_param = param
                param_dict[k] = first_param
        
            
    # nn.Sequential(nn.Linear(cross_distance_embed_dim, emb_dim), nn.ReLU(), nn.Dropout(dropout),nn.Linear(emb_dim, emb_dim))
    def reset_parameters(self):
        reset(self.k_lin)
        reset(self.q_lin)
        reset(self.v_lin)
        reset(self.out_lin)
        reset(self.edge_lin) if self.edge_lin else None
        reset(self.coords_bias_nn)
        reset(self.coords_mul_nn)
        reset(self.attn_nn) if not self.attn_nn else None 
        ones(self.skip)

    def forward(
        self,
        x: Tensor,
        edge_index: Union[Tensor,SparseTensor],
        edge_attr: Tensor,
        coords: Tensor,
    ) -> Tensor:
        r"""
        Args:
            x_dict (Dict[str, Tensor]): A dictionary holding input node
                features  for each individual node type.
            edge_index_dict (Dict[str, Union[Tensor, SparseTensor]]): A
                dictionary holding graph connectivity information for each
                individual edge type, either as a :obj:`torch.LongTensor` of
                shape :obj:`[2, num_edges]` or a
                :obj:`torch_sparse.SparseTensor`.

        :rtype: :obj:`Dict[str, Optional[Tensor]]` - The output node embeddings
            for each node type.
            In case a node type does not receive any message, its output will
            be set to :obj:`None`.
        """

        x = self.norm_input['node'](x)
        k = self.k_lin['node'](x)
        q = self.q_lin['node'](x)
        v = self.v_lin['node'](x)
        out = self.propagate(
                edge_index,
                size=None,
                k=k,
                q=q,
                v=v,
                coords=coords,
                e=edge_attr,
            )


        out = self.out_lin['node'](F.relu(out))
        out = self.norm_output['node'](out)
        identity = x
        if out.size(-1) != identity.size(-1):
            identity = self.identity_lin['node'](identity)
        # alpha = self.skip[node_type].sigmoid()
        out = out + identity

        return out

    def message(self, k_j: Tensor, q_i: Tensor, v_j: Tensor, 
                e: Tensor, coords_i: Tensor,coords_j: Tensor,
                index: Tensor, ptr: Optional[Tensor],
                size_i: Optional[int]) -> Tensor:
        delta_mul = self.coords_mul_nn(coords_i - coords_j)
        delta_bias = self.coords_bias_nn(coords_i - coords_j)
        alpha = (q_i - k_j)* delta_mul + delta_bias 
        alpha = self.attn_nn[ '__'.join(('node', 'edge', 'node'))](alpha)
        alpha = self.attn_drop(alpha)
        alpha = softmax(alpha, index, ptr, size_i)
        out = (v_j + delta_bias).view(-1, self.num_attn_groups, self.out_channels // self.num_attn_groups) * alpha.unsqueeze(-1)
        return out.view(-1, self.out_channels)

    def __repr__(self) -> str:
        return (f'{self.__class__.__name__}(-1, {self.out_channels})')

    def __repr__(self) -> str:
        return (f'{self.__class__.__name__}(-1, {self.out_channels})')
